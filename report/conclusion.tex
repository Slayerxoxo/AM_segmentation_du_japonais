%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                       CONCLUSION                         %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                Author : Coraline Marie                   %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Bien que les scores obtenus grâce à ses diverses méthodes soient satisfaisants (94.6\% de f-mesure sur le corpus
de test), il reste encore des erreurs. Ceci peut être repproché au corpus d'entrainement dont la taille reste limitée, mais pas seulement.\\

Il y a eu pendant ce projet d'autres idées qui n'ont pas eu le temps d'être implémentées, comme par exemple :
\begin{itemize}
	\item le traitement des alphabets par ngrammes ;
	\item l'ajout d'un dictionnaire : ce qui devrait éliminer les groupes de caractères inexistants ;
	\item l'entrainement du modèle de Markov caché sur d'autres types de corpus tokenisés :
	\begin{itemize}
		\item un corpus web japonais (ex : Wikipédia) avec récupération des \textit{liens} : ce qui permettrait de mettre en évidence des mots tokenisés dans leur contexte ;
		\item un corpus journalistique (ex : blogs de presse, journaux, \dots) avec reconnaissance de formes et de mise en page (titres, texte en italique, texte en gras, \dots ) ce qui pourrait également aider à la reconnaissance des mots tokenisés.
		\end{itemize}
\end{itemize}  

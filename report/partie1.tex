%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                        PARTIE 1                          %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%                Author : Coraline Marie                   %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{La segmentation par les Modèles de Markov cachés}

L'article \textit{Japanese word segmentation by hidden markov model} de Constantine P. Papageorgiou\cite{Papageorgiou:001} présente une méthode de segmentation en mots du Japonais très efficace. Cette méthode plutôt simple, utilise à la fois un corpus d'entrainement déjà segmenté et un second corpus qu'il faudra segmenter. 

La première étape de cette méthode consiste à analyser bigramme par bigramme tout le corpus d'entrainement, afin de mémoriser le comportement de chaque duo de caractères : \textit{coupure} ou \textit{non-coupure}. Cette analyse permet ensuite d'obtenir des probabilités de comportement sur l'ensemble des bigrammes rencontrés.

La seconde étape consiste à analyser bigramme par bigramme tout le corpus à segmenter, puis par l'intermédiaire d'un \textit{HMM} (Hiden Markov Model : Modèle de Marov caché), elle définit s'il faut couper ou non le bigramme.\\

Les résultats de l'implémentation de cette méthode sur le corpus de test donnent un peu plus de 89,2\% de f-mesure :
\begin{center}
	\begin{tabular}{|l c|}
	  	\hline
	  	Avg Precision & 0.904005681695 \\
		Avg Recall & 0.881382517888 \\
		Avg f-measure & 0.892550767507 \\
	  	\hline
	\end{tabular}
\end{center}


\subsection{Gestion des probabilités}

La première limite qui est observable sur cette méthode est la gestion pauvre des probabilités non observés. En effet, dans le cas où un bigramme du corpus à segmenter n'a pas été observé dans le corpus d'entrainement, la méthode attribue la même probabilité à la coupure et à la non coupure du bigramme. \\

Or, si on regarde plus attentivement les probabilités de coupure des bigrammes dans le corpus d'entrainement, on remarque qu'il est plus probable de couper que de ne pas couper un bigramme. \\

Ainsi, la première amélioration faite à cette méthode est l'attribution de deux probabilités différentes à la coupure et à la non-coupure pour les bigrammes non rencontrés. Ces probabilités sont 0.02 pour la coupure et 0.01 pour la non-coupure. Elles ont été définies pendant la phase d'entrainement, par décompte et normalisation du nombre total de bigrammes rencontrés dans le corpus d'entrainement. \\

Les résultats de cette première amélioration donnent environ 92\% de f-mesure :
\begin{center}
	\begin{tabular}{|l c|}
	  	\hline
	  	Avg Precision & 0.87553154073 \\
		Avg Recall & 0.965715790723 \\
		Avg f-measure & 0.918415054529 \\
	  	\hline
	\end{tabular}
\end{center}
